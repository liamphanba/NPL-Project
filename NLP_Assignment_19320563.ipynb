{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4638a871a58694a77473a590728ebf25047cd6eb"
   },
   "source": [
    "# NLP Assignment: Generating Trump Tweets with N-Gram Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4f13ed7359c87e397229104c1bcf18eb20603ad"
   },
   "source": [
    "In this assignment, you will use n-gram language models (LM) to model tweets (social media statements) from or about the former U.S. president Donald Trump. The goal will then be to generate new tweets, or do autocompletion, in the writing style of Trump's tweets. The tweets have been scraped from the Twitter social media (since then renamed \"X\").\n",
    "\n",
    "Before starting this assignment, the appended `NLP_ngram_cheatsheet.ipynb` notebook provides a tutorial on n-grams and LM basics, using the `nltk` package.\n",
    "\n",
    "Please code the necessary steps in python, and provide answers in Markdown format in this notebook, under the corresponding instructions and questions below.\n",
    "\n",
    "Please rename your final file `NLP_Assignment_STUDENTID.ipynb` for submission on moodle, and make sure you \"run all\" with a fresh kernel, so that outputs show correctly and in order in your submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STUDENT ID:** 19-320-563"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Import NLTK and its submodules\n",
    "import nltk\n",
    "from nltk.lm import MLE, Laplace, Lidstone\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, flatten\n",
    "from nltk.util import ngrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "\n",
    "# Import other libraries\n",
    "import random\n",
    "import re\n",
    "import string as str\n",
    "import unicodedata\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('popular', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Import, inspect and preprocess the text data\n",
    "\n",
    "- Import the provided dataset, `Trump_tweets.csv`. We are interested in the variable `Tweet_Text`, which gives the content of each tweet. \n",
    "- Before tokenizing, start by cleaning the tweets' format. You should at least normalize the different types of apostrophes and quotes (e.g. `` ’, ”, ` ``) to the corresponding ` ' ` or ` \" `, remove line breaks `\\n` (careful about not \"merging\" words), and remove multiple spacing. Also make sure urls (e.g. `https://t.co/wPk7QWpK8Z`) are not split into too many meaningless tokens. \n",
    "- (Facultative) Feel free to perform additional cleaning steps that you believe will improve the tokenization or the downstream LMs (in which case, briefly explain why).\n",
    "- Tokenize the `Tweet_Text` corpus into a list of tokenized tweets (documents). The result should be a list of lists containing word-level tokens (e.g. words, punctuation, and other \"special words\").\n",
    "- Show the result for the first five tweets of the corpus.\n",
    "\n",
    "##### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "      <th>Tweet_Text</th>\n",
       "      <th>Type</th>\n",
       "      <th>Media_Type</th>\n",
       "      <th>Hashtags</th>\n",
       "      <th>Tweet_Id</th>\n",
       "      <th>Tweet_Url</th>\n",
       "      <th>twt_favourites_IS_THIS_LIKE_QUESTION_MARK</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>15:26:37</td>\n",
       "      <td>Today we express our deepest gratitude to all ...</td>\n",
       "      <td>text</td>\n",
       "      <td>photo</td>\n",
       "      <td>ThankAVet</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>127213</td>\n",
       "      <td>41112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>13:33:35</td>\n",
       "      <td>Busy day planned in New York. Will soon be mak...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>141527</td>\n",
       "      <td>28654</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>11:14:20</td>\n",
       "      <td>Love the fact that the small groups of protest...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/797...</td>\n",
       "      <td>183729</td>\n",
       "      <td>50039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>2:19:44</td>\n",
       "      <td>Just had a very open and successful presidenti...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/796...</td>\n",
       "      <td>214001</td>\n",
       "      <td>67010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16-11-11</td>\n",
       "      <td>2:10:46</td>\n",
       "      <td>A fantastic day in D.C. Met with President Oba...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.970000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/796...</td>\n",
       "      <td>178499</td>\n",
       "      <td>36688</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7370</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>13:10:00</td>\n",
       "      <td>I loved firing goofball atheist Penn @pennjill...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.220000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>953</td>\n",
       "      <td>431</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7371</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>10:18:31</td>\n",
       "      <td>I hear @pennjillette show on Broadway is terri...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.220000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>1175</td>\n",
       "      <td>1086</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7372</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>10:10:17</td>\n",
       "      <td>Irrelevant clown @KarlRove sweats and shakes n...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.220000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>1494</td>\n",
       "      <td>930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7373</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>9:44:07</td>\n",
       "      <td>\"@HoustonWelder: Donald Trump is one of the se...</td>\n",
       "      <td>text</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.220000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>1800</td>\n",
       "      <td>1738</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374</th>\n",
       "      <td>15-07-16</td>\n",
       "      <td>0:21:25</td>\n",
       "      <td>RT @marklevinshow: Trump: Rove is a clown and ...</td>\n",
       "      <td>link</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.210000e+17</td>\n",
       "      <td>https://twitter.com/realDonaldTrump/status/621...</td>\n",
       "      <td>962</td>\n",
       "      <td>689</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7375 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      Time                                         Tweet_Text  \\\n",
       "0     16-11-11  15:26:37  Today we express our deepest gratitude to all ...   \n",
       "1     16-11-11  13:33:35  Busy day planned in New York. Will soon be mak...   \n",
       "2     16-11-11  11:14:20  Love the fact that the small groups of protest...   \n",
       "3     16-11-11   2:19:44  Just had a very open and successful presidenti...   \n",
       "4     16-11-11   2:10:46  A fantastic day in D.C. Met with President Oba...   \n",
       "...        ...       ...                                                ...   \n",
       "7370  15-07-16  13:10:00  I loved firing goofball atheist Penn @pennjill...   \n",
       "7371  15-07-16  10:18:31  I hear @pennjillette show on Broadway is terri...   \n",
       "7372  15-07-16  10:10:17  Irrelevant clown @KarlRove sweats and shakes n...   \n",
       "7373  15-07-16   9:44:07  \"@HoustonWelder: Donald Trump is one of the se...   \n",
       "7374  15-07-16   0:21:25  RT @marklevinshow: Trump: Rove is a clown and ...   \n",
       "\n",
       "      Type Media_Type   Hashtags      Tweet_Id  \\\n",
       "0     text      photo  ThankAVet  7.970000e+17   \n",
       "1     text        NaN        NaN  7.970000e+17   \n",
       "2     text        NaN        NaN  7.970000e+17   \n",
       "3     text        NaN        NaN  7.970000e+17   \n",
       "4     text        NaN        NaN  7.970000e+17   \n",
       "...    ...        ...        ...           ...   \n",
       "7370  text        NaN        NaN  6.220000e+17   \n",
       "7371  text        NaN        NaN  6.220000e+17   \n",
       "7372  text        NaN        NaN  6.220000e+17   \n",
       "7373  text        NaN        NaN  6.220000e+17   \n",
       "7374  link        NaN        NaN  6.210000e+17   \n",
       "\n",
       "                                              Tweet_Url  \\\n",
       "0     https://twitter.com/realDonaldTrump/status/797...   \n",
       "1     https://twitter.com/realDonaldTrump/status/797...   \n",
       "2     https://twitter.com/realDonaldTrump/status/797...   \n",
       "3     https://twitter.com/realDonaldTrump/status/796...   \n",
       "4     https://twitter.com/realDonaldTrump/status/796...   \n",
       "...                                                 ...   \n",
       "7370  https://twitter.com/realDonaldTrump/status/621...   \n",
       "7371  https://twitter.com/realDonaldTrump/status/621...   \n",
       "7372  https://twitter.com/realDonaldTrump/status/621...   \n",
       "7373  https://twitter.com/realDonaldTrump/status/621...   \n",
       "7374  https://twitter.com/realDonaldTrump/status/621...   \n",
       "\n",
       "      twt_favourites_IS_THIS_LIKE_QUESTION_MARK  Retweets  Unnamed: 10  \\\n",
       "0                                        127213     41112          NaN   \n",
       "1                                        141527     28654          NaN   \n",
       "2                                        183729     50039          NaN   \n",
       "3                                        214001     67010          NaN   \n",
       "4                                        178499     36688          NaN   \n",
       "...                                         ...       ...          ...   \n",
       "7370                                        953       431          NaN   \n",
       "7371                                       1175      1086          NaN   \n",
       "7372                                       1494       930          NaN   \n",
       "7373                                       1800      1738          NaN   \n",
       "7374                                        962       689          NaN   \n",
       "\n",
       "      Unnamed: 11  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  \n",
       "...           ...  \n",
       "7370          NaN  \n",
       "7371          NaN  \n",
       "7372          NaN  \n",
       "7373          NaN  \n",
       "7374          NaN  \n",
       "\n",
       "[7375 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_data = pd.read_csv('DATA/Trump_tweets.csv')\n",
    "trump_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7375 entries, 0 to 7374\n",
      "Data columns (total 12 columns):\n",
      " #   Column                                     Non-Null Count  Dtype  \n",
      "---  ------                                     --------------  -----  \n",
      " 0   Date                                       7375 non-null   object \n",
      " 1   Time                                       7375 non-null   object \n",
      " 2   Tweet_Text                                 7375 non-null   object \n",
      " 3   Type                                       7375 non-null   object \n",
      " 4   Media_Type                                 1225 non-null   object \n",
      " 5   Hashtags                                   2031 non-null   object \n",
      " 6   Tweet_Id                                   7375 non-null   float64\n",
      " 7   Tweet_Url                                  7375 non-null   object \n",
      " 8   twt_favourites_IS_THIS_LIKE_QUESTION_MARK  7375 non-null   int64  \n",
      " 9   Retweets                                   7375 non-null   int64  \n",
      " 10  Unnamed: 10                                26 non-null     float64\n",
      " 11  Unnamed: 11                                13 non-null     float64\n",
      "dtypes: float64(3), int64(2), object(7)\n",
      "memory usage: 691.5+ KB\n"
     ]
    }
   ],
   "source": [
    "trump_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select *Tweet_Tex* Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Today we express our deepest gratitude to all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Busy day planned in New York. Will soon be mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Love the fact that the small groups of protest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just had a very open and successful presidenti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A fantastic day in D.C. Met with President Oba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7370</th>\n",
       "      <td>I loved firing goofball atheist Penn @pennjill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7371</th>\n",
       "      <td>I hear @pennjillette show on Broadway is terri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7372</th>\n",
       "      <td>Irrelevant clown @KarlRove sweats and shakes n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7373</th>\n",
       "      <td>\"@HoustonWelder: Donald Trump is one of the se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7374</th>\n",
       "      <td>RT @marklevinshow: Trump: Rove is a clown and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7375 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Tweet_Text\n",
       "0     Today we express our deepest gratitude to all ...\n",
       "1     Busy day planned in New York. Will soon be mak...\n",
       "2     Love the fact that the small groups of protest...\n",
       "3     Just had a very open and successful presidenti...\n",
       "4     A fantastic day in D.C. Met with President Oba...\n",
       "...                                                 ...\n",
       "7370  I loved firing goofball atheist Penn @pennjill...\n",
       "7371  I hear @pennjillette show on Broadway is terri...\n",
       "7372  Irrelevant clown @KarlRove sweats and shakes n...\n",
       "7373  \"@HoustonWelder: Donald Trump is one of the se...\n",
       "7374  RT @marklevinshow: Trump: Rove is a clown and ...\n",
       "\n",
       "[7375 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_tweet_text = df = pd.DataFrame({'Tweet_Text': trump_data[\"Tweet_Text\"]})\n",
    "trump_tweet_text_temp = trump_tweet_text.copy()\n",
    "trump_tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retweet VS Quote Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMAGES/Differences between Retweet and Quote Tweet.png\" width=40% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truly, A. (2020). Retweets Vs. Quote Tweets: Why Twitter’s Sharing Experiment Failed. [online] ScreenRant. Available at: https://screenrant.com/twitter-retweet-vs-quote-tweet-experiment-stopped/ [Accessed 4 May 2024].\n",
    "\n",
    "‌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RT (Retweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @IvankaTrump: Such a surreal moment to vote for my father for President of the United States! Make your voice heard and vote! #Election2_'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample1 = trump_tweet_text[\"Tweet_Text\"].iloc[8]\n",
    "sample1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Retweets are generally only a copy of another tweet, displayed as a post under the user profile (here Trump), this means that the way of speech of Trump won't appear in this category of tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QT (Quote Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"@HoustonWelder: Donald Trump is one of the sexiest men on this planet. Every woman dreams of a good man who tells it like it is.\" So true!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample2 = trump_tweet_text[\"Tweet_Text\"].iloc[-2]\n",
    "sample2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Quote Tweets may contain some original Trump says, mainly comments added after a quote of someone else. Here Trump only says \"So true!\", but we want to keep this dynamic of him quoting someone else and adding is own speech. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tweets Types in our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ORIGINAL         4819\n",
       "QUOTE RETWEET    2126\n",
       "RETWEET           430\n",
       "Name: Type, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_tweet_text['Type'] = np.where(trump_tweet_text[\"Tweet_Text\"].str.startswith('RT'), 'RETWEET', \n",
    "                       np.where(trump_tweet_text[\"Tweet_Text\"].str.startswith('\"@'), 'QUOTE RETWEET', 'ORIGINAL'))\n",
    "\n",
    "trump_tweet_text[\"Type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we can identify RT by the _**RT**_ words and QT by the fact that it should always starts with  _\"@_  to mention the user we take the quote from.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Retweet and keeping Quote Retweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I believe that retaining the RETWEET feature may not be crucial in replicating Trump's tweet style. At best, it might generate tweets that he would endorse or deem worthy of sharing with his followers. However, it also risks attributing others' words to his own speech without capturing the essence of his tone or thought process. In contrast preserving Quotation Retweets could be more valuable, as they often reflect Trump's tendency to comment on or criticize existing tweets, which could help the model better understand this aspect of his behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_Text</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Today we express our deepest gratitude to all ...</td>\n",
       "      <td>ORIGINAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Busy day planned in New York. Will soon be mak...</td>\n",
       "      <td>ORIGINAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Love the fact that the small groups of protest...</td>\n",
       "      <td>ORIGINAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just had a very open and successful presidenti...</td>\n",
       "      <td>ORIGINAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A fantastic day in D.C. Met with President Oba...</td>\n",
       "      <td>ORIGINAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6940</th>\n",
       "      <td>I hope the boycott of @Macys continues forever...</td>\n",
       "      <td>ORIGINAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6941</th>\n",
       "      <td>I loved firing goofball atheist Penn @pennjill...</td>\n",
       "      <td>ORIGINAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6942</th>\n",
       "      <td>I hear @pennjillette show on Broadway is terri...</td>\n",
       "      <td>ORIGINAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6943</th>\n",
       "      <td>Irrelevant clown @KarlRove sweats and shakes n...</td>\n",
       "      <td>ORIGINAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6944</th>\n",
       "      <td>\"@HoustonWelder: Donald Trump is one of the se...</td>\n",
       "      <td>QUOTE RETWEET</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6945 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Tweet_Text           Type\n",
       "0     Today we express our deepest gratitude to all ...       ORIGINAL\n",
       "1     Busy day planned in New York. Will soon be mak...       ORIGINAL\n",
       "2     Love the fact that the small groups of protest...       ORIGINAL\n",
       "3     Just had a very open and successful presidenti...       ORIGINAL\n",
       "4     A fantastic day in D.C. Met with President Oba...       ORIGINAL\n",
       "...                                                 ...            ...\n",
       "6940  I hope the boycott of @Macys continues forever...       ORIGINAL\n",
       "6941  I loved firing goofball atheist Penn @pennjill...       ORIGINAL\n",
       "6942  I hear @pennjillette show on Broadway is terri...       ORIGINAL\n",
       "6943  Irrelevant clown @KarlRove sweats and shakes n...       ORIGINAL\n",
       "6944  \"@HoustonWelder: Donald Trump is one of the se...  QUOTE RETWEET\n",
       "\n",
       "[6945 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_tweet_text = trump_tweet_text[trump_tweet_text['Type'] != 'RETWEET']\n",
    "trump_tweet_text.reset_index(drop=True, inplace=True)\n",
    "trump_tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Therefore we removed 430 Tweets from our Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find ’ ,  ”  and ` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_occurrences = trump_tweet_text['Tweet_Text'].str.contains(r'’', regex=True)\n",
    "np.array(trump_tweet_text[pattern_occurrences][0:10][\"Tweet_Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_occurrences = trump_tweet_text['Tweet_Text'].str.contains(r'”', regex=True)\n",
    "np.array(trump_tweet_text[pattern_occurrences][0:10][\"Tweet_Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_occurrences = trump_tweet_text['Tweet_Text'].str.contains(r'`', regex=True)\n",
    "np.array(trump_tweet_text[pattern_occurrences][0:10][\"Tweet_Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is no other apostrophes type in our Tweets, except the standard \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Just out according to @CNN: \"Utah officials report voting machine problems across entire country\"',\n",
       "       '\"The Clinton Campaign at Obama Justice\" #DrainTheSwamp\\nhttps://t.co/LZkvFc071z',\n",
       "       '\"It pays to have friends in high places- like the Justice Department. Clearly the Clintons do.\"\\n#DrainTheSwamp! https://t.co/KZXB4B156M',\n",
       "       '\"@PYNance: Evangelical women live at #trumptower @pdpryor1 @CissieGLynch @SaysGabrielle https://t.co/k5kGXPR2WA\"',\n",
       "       '\"@Ravenrantz: #Billygrahams grand daughter #SupportsTrump https://t.co/sKz1SPHzDZ\"  So nice, thank you Cissy Graham Lynch!!!!',\n",
       "       '\"@slh: I follow Mr.Trump at all of his rallies by watching them on https://t.co/biseaBESvS. He is a lion-hearted warrior, who inspires hope',\n",
       "       '\"@DeplorableCBTP: \"In my mind, #DonaldTrump is the only way out of this mess.\" - #PhilRobertson of TVs #DuckDynasty\"   Thank you Phil!',\n",
       "       '\"@piersmorgan: BOMBSHELL: FBI reopening its investigation into HillaryClintons email server after new discovery!',\n",
       "       'If my people said the things about me that Podesta &amp; Hillarys people said about her, I would fire them out of self respect. \"Bad instincts\"',\n",
       "       '\"@Jmoschetti1363: @Johnatsrs1949 FBI must be outraged that their hands r tied she has no regard or t secret service, FBI, or (Dallas)police\"'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_occurrences = trump_tweet_text['Tweet_Text'].str.contains(r'\"', regex=True)\n",
    "np.array(trump_tweet_text[pattern_occurrences][0:10][\"Tweet_Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here we have our standard \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find \"\\n \" - Back Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Watching the returns at 9:45pm.\\n#ElectionNight #MAGA__ https://t.co/HfuJeRZbod',\n",
       "       'Still time to #VoteTrump!\\n#iVoted #ElectionNight https://t.co/UZtYAY1Ba6',\n",
       "       'Unbelievable evening in New Hampshire - THANK YOU! Flying to Grand Rapids, Michigan now.\\nWatch NH rally here:_ https://t.co/hP88anrfgk',\n",
       "       'America must decide between failed policies or fresh perspective, a corrupt system or an outsider\\nhttps://t.co/ll8QIW9SqW',\n",
       "       'What I Like About Trump ... and Why You Need to Vote for Him\\nhttps://t.co/6rVuDUehZq',\n",
       "       'I love you North Carolina- thank you for your amazing support! Get out and https://t.co/HfihPERFgZ tomorrow!\\nWatch:_ https://t.co/jZzfqUZNYh',\n",
       "       'Starting tomorrow its going to be #AmericaFirst! Thank you for a great morning Sarasota, Florida!\\nWatch here:_ https://t.co/ig62Kjkkvl',\n",
       "       'Thank you Minnesota! It is time to #DrainTheSwamp &amp; #MAGA!\\n#ICYMI- watch: https://t.co/fVThC7yIL6 https://t.co/e8SaXiJrxj',\n",
       "       'MONDAY - 11/7/2016\\n\\nScranton, Pennsylvania at 5:30pm.\\nhttps://t.co/BcErCtsPdF\\n\\nGrand Rapids, Michigan at 11pm._ https://t.co/pgFMLp0173',\n",
       "       'Thank you Iowa - Get out &amp; #VoteTrumpPence16!\\nhttps://t.co/HfihPERFgZ https://t.co/QsukELQmKb'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_occurrences = trump_tweet_text['Tweet_Text'].str.contains(r'\\n', regex=True)\n",
    "np.array(trump_tweet_text[pattern_occurrences][0:10][\"Tweet_Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find \"\\s\" - 2 or more Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MONDAY - 11/7/2016\\n\\nScranton, Pennsylvania at 5:30pm.\\nhttps://t.co/BcErCtsPdF\\n\\nGrand Rapids, Michigan at 11pm._ https://t.co/pgFMLp0173',\n",
       "       'Van Jones: There Is A Crack in the Blue Wall۪  It Has to Do With Trade: https://t.co/BvEF9cC7o7',\n",
       "       'JOIN ME TOMORROW!\\nMINNESOTA ۢ 2pm\\nhttps://t.co/WcgLh4prS7\\n\\nMICHIGAN ۢ 6pm\\nhttps://t.co/9BqGVKNNrt\\n\\nVIRGINIA ۢ 9:30p_ https://t.co/A1oVhCrT6t',\n",
       "       'Join me in Denver, Colorado tonight at 9:30pm: https://t.co/LJYGIK7Mri\\n\\nNEW- Scranton, Pennsylvania Monday @ 5:30pm: https://t.co/BcErCtsPdF',\n",
       "       'Join me today in Wilmington, Ohio at 4pm: https://t.co/eCLECMkYLw\\n\\nTomorrow- Tampa, Florida at 10am: https://t.co/N9380pVmuM',\n",
       "       '\"@Ravenrantz: #Billygrahams grand daughter #SupportsTrump https://t.co/sKz1SPHzDZ\"  So nice, thank you Cissy Graham Lynch!!!!',\n",
       "       'Join me in Florida tomorrow!\\n\\nMIAMIۢ12pm\\nhttps://t.co/A3X71Q6sG2\\n\\nORLANDOۢ4pm\\nhttps://t.co/6BqTVoty5C\\n\\nPENSACOLAۢ7p_ https://t.co/kEQuuJeO1B',\n",
       "       'JOIN ME TOMORROW IN FLORIDA!\\n\\nMIAMIۢ12pm\\nhttps://t.co/A3X71Q6sG2\\n\\nORLANDOۢ4pm\\nhttps://t.co/6BqTVoty5C\\n\\nPENSACOLAۢ7p_ https://t.co/kNzrAeuLZO',\n",
       "       'Join me tomorrow in Michigan!\\n\\nGrand Rapids at 12pm:\\nhttps://t.co/xFPRNF7pnF\\n\\nWarren at 3pm:\\nhttps://t.co/DREwbH7ZVd https://t.co/iyXwiP0rvd',\n",
       "       'See you tomorrow Michigan!\\n\\nGrand Rapids, MI tomorrow at noon:\\nhttps://t.co/xFPRNEPNZ5\\n\\nWarren, MI tomorrow at 3pm:_ https://t.co/UHRTgS88B5'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_occurrences = trump_tweet_text['Tweet_Text'].str.contains(r'\\s{2,}', regex=True)\n",
    "np.array(trump_tweet_text[pattern_occurrences][0:10][\"Tweet_Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> There is indeed 2 or more Spaces in our Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find \"&AMP\" - \"&\"for Ampersand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['LIVE on #Periscope: Join me for a few minutes in Pennsylvania. Get out &amp; VOTE tomorrow. LETS #MAGA!! https://t.co/Ej0LmMK3YU',\n",
       "       'Hey Missouri lets defeat Crooked Hillary &amp; @koster4missouri! Koster supports Obamacare &amp; amnesty! Vote outsider Navy SEAL @EricGreitens!',\n",
       "       'Our American comeback story begins 11/8/16. Together, we will MAKE AMERICA SAFE &amp; GREAT again for everyone! Watch:_ https://t.co/ek8Cn3CgTr',\n",
       "       'Thank you Minnesota! It is time to #DrainTheSwamp &amp; #MAGA!\\n#ICYMI- watch: https://t.co/fVThC7yIL6 https://t.co/e8SaXiJrxj',\n",
       "       'Thank you Iowa - Get out &amp; #VoteTrumpPence16!\\nhttps://t.co/HfihPERFgZ https://t.co/QsukELQmKb',\n",
       "       'Thank you Hershey, Pennsylvania. Get out &amp; VOTE on November 8th &amp; we will #MAGA! #RallyForRiley\\n#ICYMI, watch here_ https://t.co/maWukVBTr8',\n",
       "       'ICE OFFICERS WARN HILLARY IMMIGRATION PLAN WILL UNLEASH GANGS, CARTELS &amp; DRUG VIOLENCE NATIONWIDE_ https://t.co/09aSrBwQrv',\n",
       "       'Thank you NH! We will end illegal immigration, stop the drugs, deport all criminal aliens&amp;save American lives! Watc_ https://t.co/uxcazVkb32',\n",
       "       'Looking at Air Force One @ MIA. Why is he campaigning instead of creating jobs &amp; fixing Obamacare? Get back to work for the American people!',\n",
       "       'Thank you for your incredible support Wisconsin and Governor @ScottWalker! It is time to #DrainTheSwamp &amp; #MAGA!_ https://t.co/gKBkKmTudn'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_occurrences = trump_tweet_text['Tweet_Text'].str.contains(r'&\\w+;', regex=True)\n",
    "np.array(trump_tweet_text[pattern_occurrences][0:10][\"Tweet_Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find \"۝\" - Arabic End of Ayah (Unicode is U+06DD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Just out: Neera Tanden, Hillary Clinton adviser said, Israel is depressing.\\u06dd I think Israel is inspiring!',\n",
       "       'Ron Fournier: \"Clinton Used Secret Server To Protect #CircleOfEnrichment\\u06dd\\nhttps://t.co/4OGP3tPxyp',\n",
       "       'A top Clinton Foundation official said he could name 500 different examples\\u06dd of conflicts of interest.\\nhttps://t.co/rtWhdYOyq7',\n",
       "       '#CrookedHillary was at center of negotiating $12M commitment from King Mohammed VI of Morocco\\u06dd to Clinton Fdn. https://t.co/HWOQ7jQWY2',\n",
       "       'Moderator: Respectfully, you won۪t answer the pay-to-play question.\\u06dd #Debate #BigLeagueTruth',\n",
       "       '#CrookedHillary gives Obama an A\\u06dd for an economic recovery that۪s the slowest since WWII... #BigLeagueTruth_ https://t.co/wVMFHdyCu2',\n",
       "       'Hillary has called for 550% more Syrian immigrants, but won۪t even mention radical Islamic terrorists.\\u06dd #Debate_ https://t.co/Rf48XkZWbu',\n",
       "       'Moderator: Hillary paid $225,000 by a Brazilian bank for a speech that called for open borders.\\u06dd That۪s a quote! #Debate #BigLeagueTruth',\n",
       "       '@AC360: How can you unite a country if you۪ve written off tens of millions of Americans?\\u06dd #Deplorables #BigLeagueTruth #Debate',\n",
       "       'FACT ӕ on red line\\u06dd in Syria: HRC \"I wasn۪t there.\" Fact: line drawn in Aug ۪12. HRC Secy of State til Feb ۪13. https://t.co/4yZjH3TR5B'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_occurrences = trump_tweet_text['Tweet_Text'].str.contains(r'۝', regex=True)\n",
    "np.array(trump_tweet_text[pattern_occurrences][0:10][\"Tweet_Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Generally used in Arabic tweets or context in arabic countries (Israel, Egypt etc...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find \"&lt\" for < and \"&gt\"; stands for the >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['$25 Million+ raised online in just one week! RECORD WEEK. #DrainTheSwamp Today we set a bigger record. Contribute &gt;https://t.co/CZ1QmzCxwO',\n",
       "       '\"@NeilTurner_: @realDonaldTrump Cruz &amp; Rubio are scared! WATCH -&gt; https://t.co/pWjLW1QBKo https://t.co/W2r6mOzgkb\"',\n",
       "       '\"@BreitbartNews: Ratings were HUGE for @realDonaldTrumps appearance on Saturday Night Live -&gt; https://t.co/wdQXRq36yF\"',\n",
       "       '\"@SweetFreedom29: Hey @realDonaldTrump --&gt; FLASHBACK: Jeb Bush Admitted Leaky۪ Immigration Led to 9/11 http://t.co/Jmm7wd32UD #tcot\"  WOW!',\n",
       "       '\"@HardcoreRepub:  @realDonaldTrump  AMERICA will be working again. BUSINESSMAN &gt; POLITICIAN. Private sector growth above all. #Trump2016\"'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_occurrences = trump_tweet_text['Tweet_Text'].str.contains(r'&gt', regex=True)\n",
    "np.array(trump_tweet_text[pattern_occurrences][0:10][\"Tweet_Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Only stand for the character \">\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_occurrences = trump_tweet_text['Tweet_Text'].str.contains(r'&lt', regex=True)\n",
    "np.array(trump_tweet_text[pattern_occurrences][0:10][\"Tweet_Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We don't have the left \"<\" case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find URLs - https://website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Today we express our deepest gratitude to all those who have served in our armed forces. #ThankAVet https://t.co/wPk7QWpK8Z',\n",
       "       'Happy 241st birthday to the U.S. Marine Corps! Thank you for your service!! https://t.co/Lz2dhrXzo4',\n",
       "       'Watching the returns at 9:45pm.\\n#ElectionNight #MAGA__ https://t.co/HfuJeRZbod',\n",
       "       'Still time to #VoteTrump!\\n#iVoted #ElectionNight https://t.co/UZtYAY1Ba6',\n",
       "       '#ElectionDay https://t.co/MXrAxYnTjY https://t.co/FZhOncih21',\n",
       "       'We need your vote. Go to the POLLS! Lets continue this MOVEMENT! Find your poll location: https://t.co/VMUdvi1tx1_ https://t.co/zGOx74Ebhw',\n",
       "       'VOTE TODAY! Go to https://t.co/MXrAxYnTjY to find your polling location. We are going to Make America Great Again!_ https://t.co/KPQ5EY9VwQ',\n",
       "       'Today we are going to win the great state of MICHIGAN and we are going to WIN back the White House! Thank you MI!_ https://t.co/onRpEvzHrW',\n",
       "       'Unbelievable evening in New Hampshire - THANK YOU! Flying to Grand Rapids, Michigan now.\\nWatch NH rally here:_ https://t.co/hP88anrfgk',\n",
       "       'Today in Florida, I pledged to stand with the people of Cuba and Venezuela in their fight against oppression- cont: https://t.co/8eELqk2wUw'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_occurrences = trump_tweet_text['Tweet_Text'].str.contains(r'https?://\\S+', regex=True)\n",
    "np.array(trump_tweet_text[pattern_occurrences][0:10][\"Tweet_Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We find a lot of website urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find \"_\" - Underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Watching the returns at 9:45pm.\\n#ElectionNight #MAGA__ https://t.co/HfuJeRZbod',\n",
       "       'We need your vote. Go to the POLLS! Lets continue this MOVEMENT! Find your poll location: https://t.co/VMUdvi1tx1_ https://t.co/zGOx74Ebhw',\n",
       "       'VOTE TODAY! Go to https://t.co/MXrAxYnTjY to find your polling location. We are going to Make America Great Again!_ https://t.co/KPQ5EY9VwQ',\n",
       "       'Today we are going to win the great state of MICHIGAN and we are going to WIN back the White House! Thank you MI!_ https://t.co/onRpEvzHrW',\n",
       "       'Unbelievable evening in New Hampshire - THANK YOU! Flying to Grand Rapids, Michigan now.\\nWatch NH rally here:_ https://t.co/hP88anrfgk',\n",
       "       'Thank you Pennsylvania! Going to New Hampshire now and on to Michigan. Watch PA rally here: https://t.co/d29DLINGst_ https://t.co/zcH9crFIKM',\n",
       "       'I love you North Carolina- thank you for your amazing support! Get out and https://t.co/HfihPERFgZ tomorrow!\\nWatch:_ https://t.co/jZzfqUZNYh',\n",
       "       'Starting tomorrow its going to be #AmericaFirst! Thank you for a great morning Sarasota, Florida!\\nWatch here:_ https://t.co/ig62Kjkkvl',\n",
       "       'Thank you Pennsylvania- I am forever grateful for your amazing support. Lets MAKE AMERICA GREAT AGAIN! #MAGA_ https://t.co/qbcJZAzw6z',\n",
       "       'Thank you Michigan! This is a MOVEMENT that will never be seen again- its our last chance to #DrainTheSwamp! Watch_ https://t.co/cSdGkCFYRL'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern_occurrences = trump_tweet_text['Tweet_Text'].str.contains(r'_', regex=True)\n",
    "np.array(trump_tweet_text[pattern_occurrences][0:10][\"Tweet_Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"-\" underscore are often the product of a user_name type of words, or preprocessing formatting to replace \"space\" character. In this case this is difficult to choose between removing it or not, since it may remove the nature of use of this character on username or hashtag. For better generative tweets, losing the Underscore shouldn't be an issue for understanding the words used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning All "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Let's clean what we have seen !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean the tweets\n",
    "def clean_tweet(tweet):\n",
    "\n",
    "    # Normalize apostrophes and quotes\n",
    "    tweet = unicodedata.normalize('NFKD', tweet).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "    # Remove line breaks\n",
    "    tweet = tweet.replace(r'\\n', ' ')\n",
    "\n",
    "    # Remove URLs \n",
    "    tweet = re.sub(r'https?://\\S+', 'https://website', tweet)\n",
    "\n",
    "    # Remove HTML entities\n",
    "    tweet = tweet.replace(r'&lt', '').replace(r'&gt', '').replace(r'&amp', '')\n",
    "\n",
    "    # Remove underscore\n",
    "    tweet = re.sub(r\"_\", ' ', tweet)\n",
    "\n",
    "    # Remove multiple spacing\n",
    "    tweet = re.sub(r'\\s{2,}', ' ', tweet)\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Removing most characters previously encountered, except we change any URLs with the equivalent unique URL: https://website/ to keep the goal of this word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "BEFORE CLEAN: \n",
      "\n",
      " Today we express our deepest gratitude to all those who have served in our armed forces. #ThankAVet https://t.co/wPk7QWpK8Z \n",
      "\n",
      "\n",
      " AFTER CLEAN: \n",
      "\n",
      " Today we express our deepest gratitude to all those who have served in our armed forces. #ThankAVet https://website\n"
     ]
    }
   ],
   "source": [
    "trump_tweet_text = trump_tweet_text.assign(Tweet_Text_Clean=trump_tweet_text['Tweet_Text'].apply(clean_tweet))\n",
    "row = 0\n",
    "\n",
    "print(\" \\nBEFORE CLEAN: \\n\\n\",trump_tweet_text.iloc[row][0], \"\\n\\n\\n\",\"AFTER CLEAN: \\n\\n\",trump_tweet_text.iloc[row][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  On one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Thank', 'you', 'Ohio', '!', '#AmericaFirst', 'https://website']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = trump_tweet_text.iloc[800][2]\n",
    "text_tokens = nltk.casual_tokenize(text)\n",
    "text_tokens "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On whole Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_Text</th>\n",
       "      <th>Type</th>\n",
       "      <th>Tweet_Text_Clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Today we express our deepest gratitude to all ...</td>\n",
       "      <td>ORIGINAL</td>\n",
       "      <td>Today we express our deepest gratitude to all ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Busy day planned in New York. Will soon be mak...</td>\n",
       "      <td>ORIGINAL</td>\n",
       "      <td>Busy day planned in New York. Will soon be mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Love the fact that the small groups of protest...</td>\n",
       "      <td>ORIGINAL</td>\n",
       "      <td>Love the fact that the small groups of protest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Just had a very open and successful presidenti...</td>\n",
       "      <td>ORIGINAL</td>\n",
       "      <td>Just had a very open and successful presidenti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A fantastic day in D.C. Met with President Oba...</td>\n",
       "      <td>ORIGINAL</td>\n",
       "      <td>A fantastic day in D.C. Met with President Oba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6940</th>\n",
       "      <td>I hope the boycott of @Macys continues forever...</td>\n",
       "      <td>ORIGINAL</td>\n",
       "      <td>I hope the boycott of @Macys continues forever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6941</th>\n",
       "      <td>I loved firing goofball atheist Penn @pennjill...</td>\n",
       "      <td>ORIGINAL</td>\n",
       "      <td>I loved firing goofball atheist Penn @pennjill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6942</th>\n",
       "      <td>I hear @pennjillette show on Broadway is terri...</td>\n",
       "      <td>ORIGINAL</td>\n",
       "      <td>I hear @pennjillette show on Broadway is terri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6943</th>\n",
       "      <td>Irrelevant clown @KarlRove sweats and shakes n...</td>\n",
       "      <td>ORIGINAL</td>\n",
       "      <td>Irrelevant clown @KarlRove sweats and shakes n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6944</th>\n",
       "      <td>\"@HoustonWelder: Donald Trump is one of the se...</td>\n",
       "      <td>QUOTE RETWEET</td>\n",
       "      <td>\"@HoustonWelder: Donald Trump is one of the se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6945 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Tweet_Text           Type  \\\n",
       "0     Today we express our deepest gratitude to all ...       ORIGINAL   \n",
       "1     Busy day planned in New York. Will soon be mak...       ORIGINAL   \n",
       "2     Love the fact that the small groups of protest...       ORIGINAL   \n",
       "3     Just had a very open and successful presidenti...       ORIGINAL   \n",
       "4     A fantastic day in D.C. Met with President Oba...       ORIGINAL   \n",
       "...                                                 ...            ...   \n",
       "6940  I hope the boycott of @Macys continues forever...       ORIGINAL   \n",
       "6941  I loved firing goofball atheist Penn @pennjill...       ORIGINAL   \n",
       "6942  I hear @pennjillette show on Broadway is terri...       ORIGINAL   \n",
       "6943  Irrelevant clown @KarlRove sweats and shakes n...       ORIGINAL   \n",
       "6944  \"@HoustonWelder: Donald Trump is one of the se...  QUOTE RETWEET   \n",
       "\n",
       "                                       Tweet_Text_Clean  \n",
       "0     Today we express our deepest gratitude to all ...  \n",
       "1     Busy day planned in New York. Will soon be mak...  \n",
       "2     Love the fact that the small groups of protest...  \n",
       "3     Just had a very open and successful presidenti...  \n",
       "4     A fantastic day in D.C. Met with President Oba...  \n",
       "...                                                 ...  \n",
       "6940  I hope the boycott of @Macys continues forever...  \n",
       "6941  I loved firing goofball atheist Penn @pennjill...  \n",
       "6942  I hear @pennjillette show on Broadway is terri...  \n",
       "6943  Irrelevant clown @KarlRove sweats and shakes n...  \n",
       "6944  \"@HoustonWelder: Donald Trump is one of the se...  \n",
       "\n",
       "[6945 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trump_tweet_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- \n",
      "\n",
      " LINE 0 : Today we express our deepest gratitude to all those who have served in our armed forces. #ThankAVet https://website \n",
      "\n",
      " TOKENS: ['Today', 'we', 'express', 'our', 'deepest', 'gratitude', 'to', 'all', 'those', 'who', 'have', 'served', 'in', 'our', 'armed', 'forces', '.', '#ThankAVet', 'https://website'] \n",
      "\n",
      "-------------------- \n",
      "\n",
      " LINE 1 : Busy day planned in New York. Will soon be making some very important decisions on the people who will be running our government! \n",
      "\n",
      " TOKENS: ['Busy', 'day', 'planned', 'in', 'New', 'York', '.', 'Will', 'soon', 'be', 'making', 'some', 'very', 'important', 'decisions', 'on', 'the', 'people', 'who', 'will', 'be', 'running', 'our', 'government', '!'] \n",
      "\n",
      "-------------------- \n",
      "\n",
      " LINE 2 : Love the fact that the small groups of protesters last night have passion for our great country. We will all come together and be proud! \n",
      "\n",
      " TOKENS: ['Love', 'the', 'fact', 'that', 'the', 'small', 'groups', 'of', 'protesters', 'last', 'night', 'have', 'passion', 'for', 'our', 'great', 'country', '.', 'We', 'will', 'all', 'come', 'together', 'and', 'be', 'proud', '!'] \n",
      "\n",
      "-------------------- \n",
      "\n",
      " LINE 3 : Just had a very open and successful presidential election. Now professional protesters, incited by the media, are protesting. Very unfair! \n",
      "\n",
      " TOKENS: ['Just', 'had', 'a', 'very', 'open', 'and', 'successful', 'presidential', 'election', '.', 'Now', 'professional', 'protesters', ',', 'incited', 'by', 'the', 'media', ',', 'are', 'protesting', '.', 'Very', 'unfair', '!'] \n",
      "\n",
      "-------------------- \n",
      "\n",
      " LINE 4 : A fantastic day in D.C. Met with President Obama for first time. Really good meeting, great chemistry. Melania liked Mrs. O a lot! \n",
      "\n",
      " TOKENS: ['A', 'fantastic', 'day', 'in', 'D', '.', 'C', '.', 'Met', 'with', 'President', 'Obama', 'for', 'first', 'time', '.', 'Really', 'good', 'meeting', ',', 'great', 'chemistry', '.', 'Melania', 'liked', 'Mrs', '.', 'O', 'a', 'lot', '!'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "trump_tweet_text = trump_tweet_text.assign(Tweet_Tokens=trump_tweet_text[\"Tweet_Text_Clean\"].apply(nltk.casual_tokenize))\n",
    "\n",
    "# First 5 Sentences\n",
    "for i in range(5):\n",
    "    print(\"-\"*20,\"\\n\\n LINE\",i,\":\",trump_tweet_text[\"Tweet_Text_Clean\"].iloc[i], \"\\n\\n\", \"TOKENS:\",trump_tweet_text[\"Tweet_Tokens\"].iloc[i], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32c5ad89e3e61b143e940d50a0d0ee602dadfe3f"
   },
   "source": [
    "## Part 2: Fitting and Accessing a Trump Tweet LM\n",
    "\n",
    "### Ex. 2.1: LM fitting function\n",
    "Create a function that takes as arguments (at least) the desired order $n$ of the model and a tokenized training corpus, and that returns the \"simple\" Maximum Likelihood Estimator (MLE) language model, fitted on the given training corpus.  \n",
    "\n",
    "Then, use your function to fit a MLE language model of order $n=3$ to the Trump Tweets corpus.\n",
    "\n",
    "##### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Today', 'we', 'express', 'our', 'deepest', 'gratitude', 'to', 'all', 'those', 'who', 'have', 'served', 'in', 'our', 'armed', 'forces', '.', '#ThankAVet', 'https://website'] \n",
      "\n",
      "['Busy', 'day', 'planned', 'in', 'New', 'York', '.', 'Will', 'soon', 'be', 'making', 'some', 'very', 'important', 'decisions', 'on', 'the', 'people', 'who', 'will', 'be', 'running', 'our', 'government', '!'] \n",
      "\n",
      "['Love', 'the', 'fact', 'that', 'the', 'small', 'groups', 'of', 'protesters', 'last', 'night', 'have', 'passion', 'for', 'our', 'great', 'country', '.', 'We', 'will', 'all', 'come', 'together', 'and', 'be', 'proud', '!'] \n",
      "\n",
      "['Just', 'had', 'a', 'very', 'open', 'and', 'successful', 'presidential', 'election', '.', 'Now', 'professional', 'protesters', ',', 'incited', 'by', 'the', 'media', ',', 'are', 'protesting', '.', 'Very', 'unfair', '!'] \n",
      "\n",
      "['A', 'fantastic', 'day', 'in', 'D', '.', 'C', '.', 'Met', 'with', 'President', 'Obama', 'for', 'first', 'time', '.', 'Really', 'good', 'meeting', ',', 'great', 'chemistry', '.', 'Melania', 'liked', 'Mrs', '.', 'O', 'a', 'lot', '!'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "corp = trump_tweet_text[\"Tweet_Tokens\"].tolist()\n",
    "\n",
    "for i in range(5):\n",
    "    print(corp[i],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Checking that our Corpus is correct for further processing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Grams and Padded Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== n-everygram data (n=3) for first sequence in \"Corpus\": ====\n",
      "\n",
      "[('<s>',), ('<s>', '<s>'), ('<s>', '<s>', 'Today'), ('<s>',), ('<s>', 'Today'), ('<s>', 'Today', 'we'), ('Today',), ('Today', 'we'), ('Today', 'we', 'express'), ('we',), ('we', 'express'), ('we', 'express', 'our'), ('express',), ('express', 'our'), ('express', 'our', 'deepest'), ('our',), ('our', 'deepest'), ('our', 'deepest', 'gratitude'), ('deepest',), ('deepest', 'gratitude'), ('deepest', 'gratitude', 'to'), ('gratitude',), ('gratitude', 'to'), ('gratitude', 'to', 'all'), ('to',), ('to', 'all'), ('to', 'all', 'those'), ('all',), ('all', 'those'), ('all', 'those', 'who'), ('those',), ('those', 'who'), ('those', 'who', 'have'), ('who',), ('who', 'have'), ('who', 'have', 'served'), ('have',), ('have', 'served'), ('have', 'served', 'in'), ('served',), ('served', 'in'), ('served', 'in', 'our'), ('in',), ('in', 'our'), ('in', 'our', 'armed'), ('our',), ('our', 'armed'), ('our', 'armed', 'forces'), ('armed',), ('armed', 'forces'), ('armed', 'forces', '.'), ('forces',), ('forces', '.'), ('forces', '.', '#ThankAVet'), ('.',), ('.', '#ThankAVet'), ('.', '#ThankAVet', 'https://website'), ('#ThankAVet',), ('#ThankAVet', 'https://website'), ('#ThankAVet', 'https://website', '</s>'), ('https://website',), ('https://website', '</s>'), ('https://website', '</s>', '</s>'), ('</s>',), ('</s>', '</s>'), ('</s>',)]\n",
      "\n",
      "==== Vocabulary data: ====\n",
      "\n",
      "['<s>', 'Today', 'we', 'express', 'our', 'deepest', 'gratitude', 'to', 'all', 'those', 'who', 'have', 'served', 'in', 'our', 'armed', 'forces', '.', '#ThankAVet', 'https://website', '</s>', '</s>', '<s>', '<s>', 'Busy', 'day', 'planned', 'in', 'New', 'York', '.', 'Will', 'soon', 'be', 'making', 'some', 'very', 'important', 'decisions', 'on', 'the', 'people', 'who', 'will', 'be', 'running', 'our', 'government', '!', '</s>', '</s>', '<s>', '<s>', 'Love', 'the', 'fact', 'that', 'the', 'small', 'groups', 'of', 'protesters', 'last', 'night', 'have', 'passion', 'for', 'our', 'great', 'country', '.', 'We', 'will', 'all', 'come', 'together', 'and', 'be', 'proud', '!', '</s>', '</s>', '<s>', '<s>', 'Just', 'had', 'a', 'very', 'open', 'and', 'successful', 'presidential', 'election', '.', 'Now', 'professional', 'protesters', ',', 'incited']\n"
     ]
    }
   ],
   "source": [
    "training_neverygrams, padded_vocab_stream = padded_everygram_pipeline(3, corp)\n",
    "\n",
    "line_count = 0\n",
    "\n",
    "print('==== n-everygram data (n=3) for first sequence in \"Corpus\": ====\\n')\n",
    "for ngramlize_sent in training_neverygrams:\n",
    "    print(list(ngramlize_sent))\n",
    "    print()\n",
    "    line_count += 1\n",
    "    if line_count >= 1:\n",
    "        break\n",
    "\n",
    "print('==== Vocabulary data: ====\\n')\n",
    "print(list(padded_vocab_stream)[1:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Function for Fit - MLE of order 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_language_model(n, corpus, model_type='mle'):\n",
    "    \n",
    "    model_classes = {'mle': MLE, 'laplace': Laplace}\n",
    "    ModelClass = model_classes.get(model_type, MLE)\n",
    "\n",
    "    ngrams, vocab = padded_everygram_pipeline(n, corpus)\n",
    "    model = ModelClass(n)\n",
    "    model.fit(ngrams, vocab)\n",
    "\n",
    "    num_tokens_after = len(model.vocab)\n",
    "\n",
    "    print(\"\\nTokens after fitting:\", num_tokens_after)\n",
    "\n",
    "    print(\"\\n\",model)\n",
    "\n",
    "    print(\"\\nDifferences with Corpus: \", len(set(list(model.vocab))) - len(set(list(flatten(corp)))), \"( Corpus has\",len(set(list(flatten(corp)))),\")\", \"\\n\\n Tokens differences between Model and Corpus:\",set(list(model.vocab)) - set(list(flatten(corp))))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens after fitting: 13717\n",
      "\n",
      " <nltk.lm.models.MLE object at 0x15e001840>\n",
      "\n",
      "Differences with Corpus:  3 ( Corpus has 13714 ) \n",
      "\n",
      " Tokens differences between Model and Corpus: {'<UNK>', '<s>', '</s>'}\n"
     ]
    }
   ],
   "source": [
    "trump_model = train_language_model(n = 3 , corpus = corp, model_type = \"mle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex. 2.2: Vocabulary\n",
    "- How many distinct tokens are in the model's vocabulary? Is that the same number of distinct tokens that appear in the tokenized corpus?\n",
    "- Lookup the tokens of the sentence `\"I love UNIGE students!\"` in the model's vocabulary. Explain what you observe, and why. \n",
    "\n",
    "##### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens after fitting: 13717\n",
      "\n",
      " <nltk.lm.models.MLE object at 0x16938e380>\n",
      "\n",
      "Differences with Corpus:  3 ( Corpus has 13714 ) \n",
      "\n",
      " Tokens differences between Model and Corpus: {'<UNK>', '<s>', '</s>'}\n"
     ]
    }
   ],
   "source": [
    "trump_model = train_language_model(n = 3 , corpus = corp, model_type = \"mle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see that our model has 3 distinct tokens more that our original Corpus, because we actually account for the **UNK**, **s** and **/s** tokens that are used to help the model start and end the sentences and also identify which tokens where remove during the cut-off process in the vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\"I love UNIGE students!\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'love', '<UNK>', 'students', '!')\n"
     ]
    }
   ],
   "source": [
    "print(trump_model.vocab.lookup('I love UNIGE students !'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> UNIGE is not contained in the vocabulary of model MLE, thus returning the **UNK** word, we can check that by doing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"UNIGE\" in trump_model.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Which confirms that it wasn't in the original Corpus, which would have been funny to watch, Trump talking about our University or Quote someone else who did. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af5a0851a2f8707ea2e172681342ed3ecd872328"
   },
   "source": [
    "### Ex. 2.3: Token probabilities\n",
    "- When it comes to ngram models the training boils down to counting the ngrams from the training corpus. Using your fitted model, how many times do the following appear in the training data: ``'America', 'Trump', 'I will', 'will never forget'``.\n",
    "- Then, compute the following word occurrence probabilities ('scores') in the Trump Tweets corpus, and briefly explain what the returned numbers mean about the training data:\n",
    "    - $\\mathbb{P}($'America'$)$,\n",
    "    - $\\mathbb{P}($'Trump'$)$,\n",
    "    - $\\mathbb{P}($'will'$\\vert $'I'$)$,\n",
    "    - $\\mathbb{P}($'forget'$\\vert $'will never'$)$.\n",
    "- Briefly explain, with a formula, how those probabilities are obtained from the n-gram counts.\n",
    "\n",
    "##### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'America' count is: 250 and in lowercase: 2 \n",
      "'Trump' count is: 918 and in lowercase: 34 \n",
      "'I will' count is: 0 \n",
      "'will never forget' count is: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"'America' count is:\",trump_model.counts['America'], \"and in lowercase:\",trump_model.counts['america'],\"\\n'Trump' count is:\",trump_model.counts['Trump'],\"and in lowercase:\",trump_model.counts['trump'], \"\\n'I will' count is:\",trump_model.counts['I will'], \"\\n'will never forget' count is:\",trump_model.counts['will never forget'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'I' + 'will' count is: 344 \n",
      "'will never' + 'forget' count is: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n'I' + 'will' count is:\",trump_model.counts[['I']]['will'], \"\\n'will never' + 'forget' count is:\",trump_model.counts[['will never']]['forget'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'will' + 'never' + 'forget' count is: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"'will' + 'never' + 'forget' count is:\",trump_model.counts[['will', 'never']]['forget'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{P}($'America'$)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14612277820315742 %\n"
     ]
    }
   ],
   "source": [
    "print(trump_model.score('America')*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"*America*\" is approx 0.1% of the Corpus, which may counterintuitive, but Trump generally use important keywords in # hashtag, which is why only looking at \"*Amercia*\" and not \"*#makeamericagreatagain*\" or \"*#America*\" seems to be not that frequent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{P}($'america'$)$ if we also want lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011689822256252594 %\n"
     ]
    }
   ],
   "source": [
    "print(trump_model.score('america')*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> lowercase is way less present than uppercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{P}($'Trump'$)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5365628415619941 %\n"
     ]
    }
   ],
   "source": [
    "print(trump_model.score('Trump')*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"*Trump*\" word is approx 0.5% in the MLE model, which isn't surprising as we removed most Retweets, and in rare occasion Trump would actually use his own name in a sentence. What may happens more is when the Quote Tweet that Trump use mention himself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{P}($'trump'$)$ if we also want lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01987269783562941 %\n"
     ]
    }
   ],
   "source": [
    "print(trump_model.score('trump')*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> lowercase is way less present than uppercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{P}($'will'$\\vert $'I'$)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.52505966587112 %\n"
     ]
    }
   ],
   "source": [
    "print(trump_model.score('will', 'I'.split())*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> we can see that the probabiltiy rise with *I* followed by \"*will*\", since he is the person writing, and \"*will*\" is a very common word for political speech to be followed after the pronoun \"*I*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{P}($'forget'$\\vert $'will never'$)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.57142857142857 %\n"
     ]
    }
   ],
   "source": [
    "print(trump_model.score('forget', 'will never'.split())*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"*will never*\" negation followed by \"*forget*\" verb seems very common as well, Political speech again seems to often use the sentence \"*I will never forget...*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formulas for N-Grams "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From \"Introduction into NLP lecture - Slides 22 & 23\"\n",
    "\n",
    "To derive the probabilities from trigram counts, we employ the following formula:\n",
    "\n",
    "$P(w_n | w_{1:(n-1)}) ≈ P(w_n | w_{(n-2):(n-1)})$\n",
    "\n",
    "\n",
    "This equation embodies the **Markov assumption**, which posits that the probability of the current word $w_n$ is solely dependent on the preceding two words $w_{(n-2)}$ and $w_{(n-1)}$. We utilize **maximum likelihood estimation** to approximate this probability, where the transition matrix $P$ of size $V^3$ (with $V$ representing the vocabulary) is calculated from the bigram counts in the corpus.\n",
    "\n",
    "More specifically, for any two vocabulary words $v_i$ and $v_j$, the transition probability $P_{ij}$ is estimated as:\n",
    "\n",
    "$P_{ijk} = P(v_k | v_i, v_j) ≈ \\frac{C(v_i v_jv_k)}{C(v_iv_j)}$\n",
    "\n",
    "This estimation yields the probability of observing $v_k$ given that $v_i$ and $v_j$ have been previously observed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18f9e0a8d0aba302532b8a84f342d1bf4d3e202a"
   },
   "source": [
    "## Part 3: Generation using N-gram Language Model\n",
    "\n",
    "### Ex. 3.1: Tweet generator\n",
    "Create a python function to generate new Trump Tweets. It should:\n",
    "- take as input arguments: a fitted `nltk.lm.model`, a maximum number of words (integer), a text seed (initial context tokens), and a random \"RNG\" seed for generation,\n",
    "- output a newly generated Trump Tweet, according to the input arguments, post-processed as a single text string that is formatted like a tweet.\n",
    "\n",
    "*Hints:* `nltk.tokenize.treebank.TreebankWordDetokenizer()` and its `.detokenize()` method can help with post-processing. Pay attention to show things like `@user` mentions, urls, punctuation, etc... in a \"correct\" format.\n",
    "\n",
    "##### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tweet(language_model, text_seed = None, max_words = 20, random_seed = 42):\n",
    "    \"\"\"\n",
    "    Generates a sequence of words based on the given language model.\n",
    "\n",
    "    Args:\n",
    "        language_model: A trained n-gram language model.\n",
    "        max_words: The maximum number of words to generate.\n",
    "        seed: The random seed value for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        A string of generated text.\n",
    "    \"\"\"\n",
    "    generated_text = []\n",
    "    for word in language_model.generate(max_words,  text_seed, random_seed):\n",
    "        if word == '<s>':  # Skip start token\n",
    "            continue\n",
    "        if word == '</s>':  # Stop generating at end token\n",
    "            break\n",
    "        generated_text.append(word)\n",
    "\n",
    "    # Check if only one word is generated and force restart\n",
    "    if len(generated_text) <= 1:\n",
    "        return generate_tweet(language_model, text_seed, max_words, random_seed+1)\n",
    "\n",
    "    # Detokenize the generated text\n",
    "    detokenizer = nltk.tokenize.treebank.TreebankWordDetokenizer()\n",
    "    tweet = detokenizer.detokenize(generated_text)\n",
    "\n",
    "    # Capitalize the tweet\n",
    "    tweet = tweet.capitalize()\n",
    "\n",
    "    # Remove spaces before punctuations\n",
    "    for punctuation in str.punctuation:\n",
    "        if punctuation not in ['@', '#', '&']:\n",
    "            tweet = tweet.replace(\" \" + punctuation, punctuation)\n",
    "\n",
    "    while tweet and not tweet[0].isalnum():\n",
    "        tweet = tweet[1:]\n",
    "\n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generated Tweet not Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I should have easily been won against obama in 2008, not just running against the wall street'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_tweet(trump_model, max_words = 20, random_seed = 73)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generated Tweet at Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Very impressed, great to be the grand marshall- in chicago, illinois. #bigleaguetruth #debate'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_random = random.randint(0,1000)\n",
    "\n",
    "generate_tweet(trump_model, max_words = 20, random_seed = seed_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex. 3.2: Initial context\n",
    "To generate a full tweet from a LM of order $n$, explain what should be the text seed (i.e. the initial context tokens). Set the default value for the relevant argument of your function in 3.1 accordingly.\n",
    "\n",
    "##### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I win, says will vote for him and his running mate @mike pence and family hanging @monteskitchen in dutchess'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(500)\n",
    "seed_random = 2000\n",
    "\n",
    "generate_tweet(trump_model, max_words = 20,  text_seed = None, random_seed = seed_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We don't necessarly want to give a context when generating randomly some tweets by default, this will restrict the tweet generations too much. Therefore we can set text_seed to **None**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex. 3.3: Generate tweets\n",
    "Generate a few new tweets using your new function and the LM fitted in Part 2. For reproducibility, use a random RNG seed to show them. \n",
    "\n",
    "*Facultative:* show a few examples that you find interesting, representative or funny.\n",
    "\n",
    "##### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leaked the disastrous dnc e-mails, which are total losers!'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(500)\n",
    "seed_random = 1000\n",
    "\n",
    "generate_tweet(trump_model, max_words = 20,  text_seed = [\"Russia\"],random_seed = seed_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Type policy and management has done less in the debate. even paul knew it. if they dont.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(500)\n",
    "seed_random = 23\n",
    "\n",
    "generate_tweet(trump_model, max_words = 20,  text_seed = [\"Hillary\"],random_seed = seed_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Supporter @alisonforky declare crooked hillary cant even send emails without putting entire nation at risk?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(500)\n",
    "seed_random = 20\n",
    "\n",
    "generate_tweet(trump_model, max_words = 20,  text_seed = [\"Clinton\"],random_seed = seed_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Smoothing and model comparison\n",
    "\n",
    "### Ex. 4.1: Smoothed LM alternatives to simple MLE\n",
    "Modify the function that you defined in 2.1 by adding an argument that allows changing the `nltk.lm` language model that is fitted in the function (e.g. to fit a Laplace or a Lidstone model instead of the simple MLE). \n",
    "Also briefly explain what is the difference between Laplace, Lidstone and the simple MLE language models.\n",
    "\n",
    "*Hint:* Your function might need more than a single additional argument, if some LM have hyperparameters.\n",
    "\n",
    "##### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main differences between **MLE**, **Laplace Smoothing**, and **Lidstone Smoothing** are as follows:\n",
    "\n",
    "When it comes to handling unseen words, **MLE** assigns zero probability to them, which can lead to issues when encountering new words, whereas **Laplace Smoothing** adds a small constant value to each word's count, ensuring no word has zero probability, even if unseen, and **Lidstone Smoothing** also adds a fraction of the smoothing parameter to each word's count, allowing for customizable smoothing.\n",
    "\n",
    "In terms of probability distribution, **MLE** estimates probability based on observed frequency, without considering unseen events, whereas **Laplace Smoothing** distributes the added probability mass evenly among all words, including unseen words, and **Lidstone Smoothing** allows for a customizable smoothing parameter, which controls the amount of smoothing applied.\n",
    "\n",
    "Regarding bias and overestimation, **MLE** has no bias, but may struggle with unseen words, whereas **Laplace Smoothing** introduces a bias towards unseen events and can overestimate rare events, and **Lidstone Smoothing** offers more flexibility in controlling the bias and overestimation, depending on the chosen smoothing parameter.\n",
    "\n",
    "In terms of flexibility, **MLE** has no flexibility, as it relies solely on observed frequencies, whereas **Laplace Smoothing** has limited flexibility, as it adds a fixed constant value, and **Lidstone Smoothing** offers the most flexibility, as it allows for a customizable smoothing parameter.\n",
    "\n",
    "Finally, when it comes to tunability, **MLE** has no tunable parameters, whereas **Laplace Smoothing** has no tunable parameters, as the smoothing value is fixed, and **Lidstone Smoothing** allows the smoothing parameter ($\\gamma$) to be tuned based on the specific characteristics of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_language_models(n, corpus, model_type='mle', gamma = 0.1):\n",
    "    \"\"\"\n",
    "    Train a language model using maximum likelihood estimation (MLE),\n",
    "    Laplace smoothing, or Lidstone smoothing.\n",
    "\n",
    "    Args:\n",
    "        n (int): The order of the language model.\n",
    "        corpus (list): The training corpus as a list of sentences or tokens.\n",
    "        model_type (str, optional): The type of model to train.\n",
    "            Valid options: 'mle' (default), 'laplace', 'lidstone'.\n",
    "        alpha (float, optional): The gamma Lidstone smoothing.\n",
    "            Defaults to 0.1\n",
    "\n",
    "    Returns:\n",
    "        object: The trained language model.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid model_type is specified.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    model_classes = {'mle': MLE, 'laplace': Laplace, 'lidstone': Lidstone}\n",
    "    \n",
    "    ModelClass = model_classes.get(model_type)\n",
    "\n",
    "    ngrams, vocab = padded_everygram_pipeline(n, text = corpus)\n",
    "\n",
    "    if model_type == \"lidstone\": \n",
    "        model = ModelClass(order = n, gamma = gamma)\n",
    "    else:\n",
    "        model = ModelClass(order = n)\n",
    "\n",
    "    model.fit(text = ngrams, vocabulary_text = vocab)\n",
    "\n",
    "    num_tokens_after = len(model.vocab)\n",
    "\n",
    "    print(\"\\nTokens after fitting:\", num_tokens_after)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex. 4.2: Qualitative model comparison \n",
    "With $n=1,2,3,4$, fit and generate new tweets from the simple MLE and from the Laplace LM of orders $n$. \n",
    "- Compare the results between the different $n$ values and between the two models. \n",
    "- What are the main differences for generation? Which model(s) do you think might be the best options for generating new realistic tweets?\n",
    "- Do you see hints of those differences in the generated tweets?\n",
    "\n",
    "##### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting n = 1,2,3,4 for MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE MODEL\n",
      "--------------\n",
      "\n",
      "Tokens after fitting: 13715\n",
      "Order: 1 \n",
      "Tweet: Our was do @realdonaldtrump need nh out i debate one! house rnc cruz to #iacaucus vp explains.- \n",
      "-------------\n",
      "\n",
      "Tokens after fitting: 13717\n",
      "Order: 2 \n",
      "Tweet: Of weeks i dont want our people are not run! \n",
      "-------------\n",
      "\n",
      "Tokens after fitting: 13717\n",
      "Order: 3 \n",
      "Tweet: Never will.\" great! \n",
      "-------------\n",
      "\n",
      "Tokens after fitting: 13717\n",
      "Order: 4 \n",
      "Tweet: Me was the highest rated show that they have long dreamed of- and no effective raise in years. \n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "randomseed1 = 500\n",
    "\n",
    "print(\"MLE MODEL\\n--------------\")\n",
    "for i in range(1,5):\n",
    "   print(\"Order:\" , i, \"\\nTweet:\", generate_tweet(train_language_models(n = i, corpus = corp, model_type='mle'), max_words = 20,  text_seed = None, random_seed = randomseed1),\"\\n-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting n = 1,2,3,4 for LaPlace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LAPLACE MODEL\n",
      "--------------\n",
      "\n",
      "Tokens after fitting: 13715\n",
      "Order: 1 \n",
      "Tweet: Open w dangerous @rnull65 movement matter our i consistent on! heading party contract to #makeamericagreatagain trump dumb.. \n",
      "-------------\n",
      "\n",
      "Tokens after fitting: 13717\n",
      "Order: 2 \n",
      "Tweet: Of ways she ever run against me and lost so! \n",
      "-------------\n",
      "\n",
      "Tokens after fitting: 13717\n",
      "Order: 3 \n",
      "Tweet: Nations to pay fair taxes, while trump solidifies his dominating lead https://website \n",
      "-------------\n",
      "\n",
      "Tokens after fitting: 13717\n",
      "Order: 4 \n",
      "Tweet: Match for putin or if the truth be told even for hilary. usa needs a winner.\" \n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "randomseed1 = 500\n",
    "\n",
    "print(\"LAPLACE MODEL\\n--------------\")\n",
    "for i in range(1,5):\n",
    "   print(\"Order:\" , i, \"\\nTweet:\", generate_tweet(train_language_models(n = i, corpus = corp,  model_type = \"laplace\"), max_words = 20,  text_seed = None, random_seed = randomseed1),\"\\n-------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compare the results between the different $n$ values and between the two models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As we increase the order of the Model, in both case, tweets generated looks nicer, with less weird artefacts and unlogical sentences. **LaPlace** Model seems to be more often coherent than **MLE**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What are the main differences for generation? Which model(s) do you think might be the best options for generating new realistic tweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **MLE** tends to generate very close words together based on observed frequency on the Corpus, which when the order is low, is very visible. On the opposite, even on same order, **LaPlace** seems to chose more general words that don't seems to follow the n-grams order too much and looks \"smoother\" and is less predictable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Do you see hints of those differences in the generated tweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The 4th tweet of both model (order = 4) is a good example of how **MLE** is choosing very close words every 4 grams, but it looks like there is a unlogical blend between each. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex. 4.3: Quantitative evaluation and comparison\n",
    "- Split the tokenized Trump Tweets corpus into a (reproducible) training set (80%) and a test set (20%). \n",
    "- Compute the train and test 3-gram perplexity scores of a simple MLE LM, a Laplace LM, and a Lidstone LM with $\\gamma=0.1$. Use model order $n=3$ for each.\n",
    "- Compare and discuss the obtained train and test perplexity scores of the three models. Argue which model might represent the Trump Tweets data best.\n",
    "\n",
    "*Hint:* To compute the perplexity correctly, you might need to preprocess the relevant corpus documents to a list of padded $n$-grams.\n",
    "\n",
    "##### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Corpus into Training and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Train Corpus:\n",
      "\n",
      " [['Thank', 'you', 'for', 'the', 'kind', 'words', 'tonight', ',', '@OMAROSA', '.', 'You', 'were', 'great', '!', 'See', 'you', 'soon', '!'], ['I', 'will', 'do', 'far', 'more', 'for', 'women', 'than', 'Hillary', ',', 'and', 'I', 'will', 'keep', 'our', 'country', 'safe', ',', 'something', 'which', 'she', 'will', 'not', 'be', 'able', 'to', 'do-no', 'strength', '/', 'stamina', '!'], ['#MakeAmericaSafeAgain', '#ImWithYou', 'https://website'], ['\"', '@teed', 'chris', ':', '@Loyal2Trump2016', '@TrumpAlabama', '@FoxNews', 'Look', 'when', 'you', 'try', 'to', 'kill', 'Your', 'mom', ',', 'thats', 'it', 'for', 'me', ',', 'no', 'walking', 'on', 'water', '\"'], ['New', 'Iowa', 'poll', '.', 'Thank', 'you', '!', '#MakeAmericaGreatAgain', '#Trump2016', 'https://website'], ['\"', '@sparkey03', ':', '@realDonaldTrump', 'Go', '#Trump2016', '\"'], ['.', '@megynkelly', 'must', 'have', 'had', 'a', 'terrible', 'vacation', ',', 'she', 'is', 'really', 'off', 'her', 'game', '.', 'Was', 'afraid', 'to', 'confront', 'Dr', '.', 'Cornel', 'West', '.', 'No', 'clue', 'on', 'immigration', '!'], ['\"', '@Knight276', ':', '@realDonaldTrump', '@kpdelbridge', '@seanhannity', 'Im', 'a', 'veteran', '.', 'Im', 'not', 'offended', 'by', 'Trump', '\"'], ['\"', '@BOSSYtxmar55', ':', '#Trump', 'WILL', 'WIN', '!', 'Hes', 'counting', 'on', 'ALL', 'supporters', 'TO', 'VOTE', 'Then', '\"', 'WE', '\"', 'CELEBRATE', 'WITH', 'TAILGATE', 'STYLE', 'PARTIES', '!', '#TeamTrump']] \n",
      "\n",
      "---------------- \n",
      "\n",
      " Test_Corpus:\n",
      "\n",
      " [['Will', 'be', 'interviewed', 'on', '@GMA', 'at', '7:00', 'A', '.', 'M', '.', 'Big', 'wins', 'last', 'night', '!'], ['Thank', 'you', 'Windham', ',', 'New', 'Hampshire', '!', '#TrumpPence16', '#MAGA', 'https://website'], ['Tennessee', 'GOP', 'Poll', 'https://website', 'Trump', '32.7', '%', 'Cruz', '16.5', '%', 'Carson', '6.6', '%', 'Rubio', '5.3', '%', 'Christie', '2.4', '%', 'Jeb', '1.6', '%'], ['\"', '@Good2bqueen67', ':', '@brithume', '@megynkelly', 'Theyre', 'too', 'lame', 'to', 'get', 'it', ',', 'Sir', '.', 'When', 'you', 'win', 'the', 'election', 'theyll', 'get', 'it', '.', '\"'], ['.', '@THR', '\"', 'The', 'Donald', 'Trump', 'Ratings', 'Bump', ':', 'Whos', 'Benefiting', 'Most', '?', '\"', 'https://website'], ['Just', 'had', 'a', 'very', 'nice', 'meeting', 'with', '@Reince', 'Priebus', 'and', 'the', '@GOP', '.', 'Looking', 'forward', 'to', 'bringing', 'the', 'Party', 'together', '-', '-', '-', 'and', 'it', 'will', 'happen', '!'], ['\"', '@lainey34210', ':', '@realDonaldTrump', 'Great', 'opening', 'Pence', '\"'], ['\"', '@BBCARKING', ':', '@JoeTrippi', '@realDonaldTrump', '@scottienhughes', 'Joe', 'have', 'U', 'resigned', 'to', 'the', 'inevitable', '?', 'Take', 'UR', 'seat', 'on', 'the', 'https://website'], ['Will', 'be', 'on', '@CNBC', 'at', '@7', ':', '22', '.', 'Enjoy', '!']]\n",
      "\n",
      "-------------------------------\n",
      "\n",
      " 5556 (80% Train) + 1389 (20% Test) = 6945 \n",
      "\n",
      " Total Corpus: 6945\n"
     ]
    }
   ],
   "source": [
    "Train_Corpus, Test_Corpus = train_test_split(corp, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"\\n Train Corpus:\\n\\n\",Train_Corpus[1:10], \"\\n\\n----------------\",\"\\n\\n Test_Corpus:\\n\\n\",Test_Corpus[1:10])\n",
    "\n",
    "print(\"\\n-------------------------------\\n\\n\",len(Train_Corpus),\"(80% Train)\", \"+\",len(Test_Corpus),\"(20% Test)\", \"=\", len(Train_Corpus)+len(Test_Corpus), \"\\n\\n Total Corpus:\", len(corp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequence of Padded Ngram Tuples for Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to documentation for Entropy, we should prepare our Train and Test corpus into N-grams padded as tuples to compute Perplexity (as it uses Entropy function).\n",
    "\n",
    "This should look like this:\n",
    "\n",
    "corpus = [[\"I\", \"love\", \"natural\", \"language\", \"processing\"], [\"This\", \"is\", \"another\", \"sentence\"]]\n",
    "\n",
    "https://www.nltk.org/_modules/nltk/lm/api.html#LanguageModel.entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OUTPUT:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('<s>', '<s>', 'I'),\n",
       " ('<s>', 'I', 'love'),\n",
       " ('I', 'love', 'natural'),\n",
       " ('love', 'natural', 'language'),\n",
       " ('natural', 'language', 'processing'),\n",
       " ('language', 'processing', '</s>'),\n",
       " ('processing', '</s>', '</s>'),\n",
       " ('<s>', '<s>', 'This'),\n",
       " ('<s>', 'This', 'is'),\n",
       " ('This', 'is', 'another'),\n",
       " ('is', 'another', 'sentence'),\n",
       " ('another', 'sentence', '</s>'),\n",
       " ('sentence', '</s>', '</s>')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 3\n",
    "corpus_temp = [[\"I\", \"love\", \"natural\", \"language\", \"processing\"], [\"This\", \"is\", \"another\", \"sentence\"]]\n",
    "print(\"\\nOUTPUT:\")\n",
    "[tuple(ngram) for sentence in corpus_temp for ngram in ngrams(pad_both_ends(sentence, n=n), n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLE LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Order\n",
    "n = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens after fitting: 12136\n",
      "Model: <nltk.lm.models.MLE object at 0x16e048640>\n"
     ]
    }
   ],
   "source": [
    "Train_MLE = train_language_models(n = n, corpus = Train_Corpus,  model_type = \"mle\")\n",
    "print(\"Model:\",Train_MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                Perplexity on Train             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.117910105346997"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_temp = Train_Corpus\n",
    "\n",
    "ngram_corpus =  [tuple(ngram) for sentence in corpus_temp for ngram in ngrams(pad_both_ends(sentence, n=n), n=n)]\n",
    "\n",
    "Train_MLE.perplexity(ngram_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We have a very low Perplexity, which may indicate an overfit of MLE on Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                Perplexity on Test             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_temp = Test_Corpus\n",
    "\n",
    "ngram_corpus =  [tuple(ngram) for sentence in corpus_temp for ngram in ngrams(pad_both_ends(sentence, n=n), n=n)]\n",
    "\n",
    "Train_MLE.perplexity(ngram_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This should be normal to have inf as we are dealing with unkown word and the log(0) when frequency of a word is none, return -inf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Laplace LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens after fitting: 12136\n",
      "Model: <nltk.lm.models.Laplace object at 0x16de84310>\n"
     ]
    }
   ],
   "source": [
    "Train_LAPLACE = train_language_models(n = n, corpus = Train_Corpus,  model_type = \"laplace\")\n",
    "print(\"Model:\",Train_LAPLACE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                Perplexity on Train          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2752.4119200804357"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_temp = Train_Corpus\n",
    "\n",
    "ngram_corpus =  [tuple(ngram) for sentence in corpus_temp for ngram in ngrams(pad_both_ends(sentence, n=n), n=n)]\n",
    "\n",
    "Train_LAPLACE.perplexity(ngram_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We are less over fitting on Train with LaPlace this time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                Perplexity on Test             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4571.644399941378"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_temp = Test_Corpus\n",
    "\n",
    "ngram_corpus =  [tuple(ngram) for sentence in corpus_temp for ngram in ngrams(pad_both_ends(sentence, n=n), n=n)]\n",
    "\n",
    "Train_LAPLACE.perplexity(ngram_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Test should be higher with its Perplexity as this set should be unseen by our model and thus perform less good in term of probabilities and patterns and be more confused with what next words he should put. This is the case here, and also on others models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lidstone LM with $\\gamma=0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens after fitting: 12136\n",
      "Model: <nltk.lm.models.Lidstone object at 0x16cbe16c0> \n",
      "Gamma: 0.1\n"
     ]
    }
   ],
   "source": [
    "Train_LIDSTONE = train_language_models(n = n, corpus = Train_Corpus,  model_type = \"lidstone\", gamma = 0.1)\n",
    "print(\"Model:\",Train_LIDSTONE, \"\\nGamma:\",Train_LIDSTONE.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                Perplexity on Train             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "481.3861204952867"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_temp = Train_Corpus\n",
    "\n",
    "ngram_corpus =  [tuple(ngram) for sentence in corpus_temp for ngram in ngrams(pad_both_ends(sentence, n=n), n=n)]\n",
    "\n",
    "Train_LIDSTONE.perplexity(ngram_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The smaller perplexity obtained with a lower gamma suggests that the Lidstone smoothing parameter is giving more weight to the observed words in the training set, potentially indicating a stronger reliance on the training data and a higher susceptibility to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                Perplexity on Test             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2491.5433199218714"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_temp = Test_Corpus\n",
    "\n",
    "ngram_corpus =  [tuple(ngram) for sentence in corpus_temp for ngram in ngrams(pad_both_ends(sentence, n=n), n=n)]\n",
    "\n",
    "Train_LIDSTONE.perplexity(ngram_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Compared to LaPlace Train VS Test, our Test set on Lidstone with $\\gamma$ = 0.1 is proportionally way higher, since we may have overfit our Train Corpus. This is very difficult to choose which Model is best only based on the same Order and without Hyper-Parameter Tuning, but solely based on the Perplexity score we obtain from all models at $n$ = 3, Lidstone with $\\gamma$ = 0.1 seems to be less prone to overfitting on the Test set and would performs generally better on unseen Corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ex. 4.4: Hyper-parameter tuning\n",
    "- Perform a grid-search to select the best hyperparameter values for $n$ and $\\gamma$, for the Lidstone LM. You want to select the model that generalizes best to new data.\n",
    "- What do you observe in the obtained perplexity scores? Was it expected? Explain it in statistical terms.\n",
    "\n",
    "*Hint:* Maybe try a few values for $n$ and $\\gamma$ by hand to identify the general hyperparameter region of interest before defining a more thorough hyperparameter value grid.\n",
    "\n",
    "##### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_language_models_grid(n, corpus, model_type='mle', gamma = 0.1):\n",
    "    \"\"\"\n",
    "    Train a language model using maximum likelihood estimation (MLE),\n",
    "    Laplace smoothing, or Lidstone smoothing.\n",
    "\n",
    "    Args:\n",
    "        n (int): The order of the language model.\n",
    "        corpus (list): The training corpus as a list of sentences or tokens.\n",
    "        model_type (str, optional): The type of model to train.\n",
    "            Valid options: 'mle' (default), 'laplace', 'lidstone'.\n",
    "        alpha (float, optional): The gamma Lidstone smoothing.\n",
    "            Defaults to 0.1\n",
    "\n",
    "    Returns:\n",
    "        object: The trained language model.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an invalid model_type is specified.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    model_classes = {'mle': MLE, 'laplace': Laplace, 'lidstone': Lidstone}\n",
    "    \n",
    "    ModelClass = model_classes.get(model_type)\n",
    "\n",
    "    ngrams, vocab = padded_everygram_pipeline(n, text = corpus)\n",
    "\n",
    "    if model_type == \"lidstone\": \n",
    "        model = ModelClass(order = n, gamma = gamma)\n",
    "    else:\n",
    "        model = ModelClass(order = n)\n",
    "\n",
    "    model.fit(text = ngrams, vocabulary_text = vocab)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Broad Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Testing for multiple $n$ and $\\gamma$ before restricting our search.\n",
    ">\n",
    "> We try to increment $\\gamma$ by 0.1 and go over 1 to 5 for $n$, testing the perplexity score on the Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamters Init\n",
    "order_max = 5\n",
    "gamma_min = 0\n",
    "gamma_max = 1.5\n",
    "gamma_increment = 0.1\n",
    "\n",
    "# Score Init\n",
    "best_model_perplexity = 10**10\n",
    "best_model_order = 0\n",
    "best_model_gamma = 0\n",
    "\n",
    "# Store each Values\n",
    "results_array = np.empty((0, 3))\n",
    "\n",
    "# Grid Search\n",
    "for order in range(1,order_max+1):\n",
    "\n",
    "    for gamma in np.arange(gamma_min+gamma_increment, gamma_max, gamma_increment).round(10):\n",
    "\n",
    "        model = train_language_models_grid(n = order, corpus = Train_Corpus,  model_type = \"lidstone\", gamma = gamma)\n",
    "        \n",
    "        perplexity = model.perplexity([tuple(ngram) for sentence in Test_Corpus for ngram in ngrams(pad_both_ends(sentence, n=order), n=order)])\n",
    "\n",
    "        new_row = np.array([[order, gamma, perplexity]])\n",
    "        results_array = np.append(results_array, new_row, axis=0)\n",
    "\n",
    "        if perplexity < best_model_perplexity:\n",
    "           best_model_perplexity = perplexity\n",
    "           best_model_order = order\n",
    "           best_model_gamma = gamma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Hyperparameters and Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_ce9ec_row5_col0, #T_ce9ec_row5_col1, #T_ce9ec_row5_col2 {\n",
       "  background-color: #a87d4c;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_ce9ec\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ce9ec_level0_col0\" class=\"col_heading level0 col0\" >order</th>\n",
       "      <th id=\"T_ce9ec_level0_col1\" class=\"col_heading level0 col1\" >gamma</th>\n",
       "      <th id=\"T_ce9ec_level0_col2\" class=\"col_heading level0 col2\" >perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ce9ec_level0_row0\" class=\"row_heading level0 row0\" >9</th>\n",
       "      <td id=\"T_ce9ec_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_ce9ec_row0_col1\" class=\"data row0 col1\" >1.000000</td>\n",
       "      <td id=\"T_ce9ec_row0_col2\" class=\"data row0 col2\" >937.394128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ce9ec_level0_row1\" class=\"row_heading level0 row1\" >10</th>\n",
       "      <td id=\"T_ce9ec_row1_col0\" class=\"data row1 col0\" >1</td>\n",
       "      <td id=\"T_ce9ec_row1_col1\" class=\"data row1 col1\" >1.100000</td>\n",
       "      <td id=\"T_ce9ec_row1_col2\" class=\"data row1 col2\" >937.047833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ce9ec_level0_row2\" class=\"row_heading level0 row2\" >11</th>\n",
       "      <td id=\"T_ce9ec_row2_col0\" class=\"data row2 col0\" >1</td>\n",
       "      <td id=\"T_ce9ec_row2_col1\" class=\"data row2 col1\" >1.200000</td>\n",
       "      <td id=\"T_ce9ec_row2_col2\" class=\"data row2 col2\" >937.174801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ce9ec_level0_row3\" class=\"row_heading level0 row3\" >12</th>\n",
       "      <td id=\"T_ce9ec_row3_col0\" class=\"data row3 col0\" >1</td>\n",
       "      <td id=\"T_ce9ec_row3_col1\" class=\"data row3 col1\" >1.300000</td>\n",
       "      <td id=\"T_ce9ec_row3_col2\" class=\"data row3 col2\" >937.696749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ce9ec_level0_row4\" class=\"row_heading level0 row4\" >13</th>\n",
       "      <td id=\"T_ce9ec_row4_col0\" class=\"data row4 col0\" >1</td>\n",
       "      <td id=\"T_ce9ec_row4_col1\" class=\"data row4 col1\" >1.400000</td>\n",
       "      <td id=\"T_ce9ec_row4_col2\" class=\"data row4 col2\" >938.552803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ce9ec_level0_row5\" class=\"row_heading level0 row5\" >14</th>\n",
       "      <td id=\"T_ce9ec_row5_col0\" class=\"data row5 col0\" >2</td>\n",
       "      <td id=\"T_ce9ec_row5_col1\" class=\"data row5 col1\" >0.100000</td>\n",
       "      <td id=\"T_ce9ec_row5_col2\" class=\"data row5 col2\" >791.432359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ce9ec_level0_row6\" class=\"row_heading level0 row6\" >15</th>\n",
       "      <td id=\"T_ce9ec_row6_col0\" class=\"data row6 col0\" >2</td>\n",
       "      <td id=\"T_ce9ec_row6_col1\" class=\"data row6 col1\" >0.200000</td>\n",
       "      <td id=\"T_ce9ec_row6_col2\" class=\"data row6 col2\" >1020.758654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ce9ec_level0_row7\" class=\"row_heading level0 row7\" >16</th>\n",
       "      <td id=\"T_ce9ec_row7_col0\" class=\"data row7 col0\" >2</td>\n",
       "      <td id=\"T_ce9ec_row7_col1\" class=\"data row7 col1\" >0.300000</td>\n",
       "      <td id=\"T_ce9ec_row7_col2\" class=\"data row7 col2\" >1206.490820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ce9ec_level0_row8\" class=\"row_heading level0 row8\" >17</th>\n",
       "      <td id=\"T_ce9ec_row8_col0\" class=\"data row8 col0\" >2</td>\n",
       "      <td id=\"T_ce9ec_row8_col1\" class=\"data row8 col1\" >0.400000</td>\n",
       "      <td id=\"T_ce9ec_row8_col2\" class=\"data row8 col2\" >1366.983337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ce9ec_level0_row9\" class=\"row_heading level0 row9\" >18</th>\n",
       "      <td id=\"T_ce9ec_row9_col0\" class=\"data row9 col0\" >2</td>\n",
       "      <td id=\"T_ce9ec_row9_col1\" class=\"data row9 col1\" >0.500000</td>\n",
       "      <td id=\"T_ce9ec_row9_col2\" class=\"data row9 col2\" >1510.048510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ce9ec_level0_row10\" class=\"row_heading level0 row10\" >19</th>\n",
       "      <td id=\"T_ce9ec_row10_col0\" class=\"data row10 col0\" >2</td>\n",
       "      <td id=\"T_ce9ec_row10_col1\" class=\"data row10 col1\" >0.600000</td>\n",
       "      <td id=\"T_ce9ec_row10_col2\" class=\"data row10 col2\" >1639.986642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x17c382ce0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_lowest_perplexity(row):\n",
    "    if row['perplexity'] == results_df['perplexity'].min():\n",
    "        return ['background-color: #a87d4c'] * len(row)\n",
    "    else:\n",
    "        return [''] * len(row)\n",
    "    \n",
    "results_df = pd.DataFrame(results_array, columns=['order', 'gamma', 'perplexity'])\n",
    "results_df.style.apply(highlight_lowest_perplexity, axis=1)\n",
    "results_df[\"order\"] = results_df[\"order\"].astype(int)\n",
    "best_index = results_df['perplexity'].idxmin()\n",
    "\n",
    "# Select the 10 rows around the best row\n",
    "slice_start = max(0, best_index - 5)\n",
    "slice_end = min(len(results_df), best_index + 6)\n",
    "results_df_slice = results_df.iloc[slice_start:slice_end]\n",
    "\n",
    "# Apply the styling function to the sliced DataFrame\n",
    "styled_df_slice = results_df_slice.style.apply(highlight_lowest_perplexity, axis=1)\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styled_df_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Highlighted row in this table is our best Perplexity on Test, which shows that Lidstone of $n$ = 2 and $\\gamma$ = 0.1 seems the best for our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "order=1<br>gamma=%{x}<br>perplexity=%{y}<extra></extra>",
         "legendgroup": "1",
         "line": {
          "color": "#636EFA",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "1",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0.1,
          0.2,
          0.3,
          0.4,
          0.5,
          0.6,
          0.7,
          0.8,
          0.9,
          1,
          1.1,
          1.2,
          1.3,
          1.4
         ],
         "xaxis": "x",
         "y": [
          1026.051883179268,
          989.7421015282621,
          971.0649303347037,
          959.4472196437375,
          951.6517670296951,
          946.2555733936476,
          942.5057843102148,
          939.9553304699904,
          938.3167526497882,
          937.3941278185977,
          937.0478331188679,
          937.1748012872085,
          937.6967494526996,
          938.5528027917402
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "order=2<br>gamma=%{x}<br>perplexity=%{y}<extra></extra>",
         "legendgroup": "2",
         "line": {
          "color": "#EF553B",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "2",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0.1,
          0.2,
          0.3,
          0.4,
          0.5,
          0.6,
          0.7,
          0.8,
          0.9,
          1,
          1.1,
          1.2,
          1.3,
          1.4
         ],
         "xaxis": "x",
         "y": [
          791.4323585225626,
          1020.758654021152,
          1206.4908196407528,
          1366.983337287597,
          1510.0485103835506,
          1639.9866420607461,
          1759.5083783079317,
          1870.4723097302751,
          1974.227964284384,
          2071.796790045285,
          2163.9766436126774,
          2251.406323363497,
          2334.607570744279,
          2414.0135763758794
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "order=3<br>gamma=%{x}<br>perplexity=%{y}<extra></extra>",
         "legendgroup": "3",
         "line": {
          "color": "#00CC96",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "3",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0.1,
          0.2,
          0.3,
          0.4,
          0.5,
          0.6,
          0.7,
          0.8,
          0.9,
          1,
          1.1,
          1.2,
          1.3,
          1.4
         ],
         "xaxis": "x",
         "y": [
          2491.5433199218714,
          2984.8320972176302,
          3326.9982622912203,
          3594.3558633717184,
          3815.471500148353,
          4004.667782917914,
          4170.302021265309,
          4317.735069857576,
          4450.63725638352,
          4571.644399941378,
          4682.719916611924,
          4785.369330300476,
          4880.774548827671,
          4969.881718574254
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "order=4<br>gamma=%{x}<br>perplexity=%{y}<extra></extra>",
         "legendgroup": "4",
         "line": {
          "color": "#AB63FA",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "4",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0.1,
          0.2,
          0.3,
          0.4,
          0.5,
          0.6,
          0.7,
          0.8,
          0.9,
          1,
          1.1,
          1.2,
          1.3,
          1.4
         ],
         "xaxis": "x",
         "y": [
          3744.108482258509,
          4206.363658994775,
          4516.03474333417,
          4753.195350030476,
          4946.654660917262,
          5110.500690715401,
          5252.8016082508775,
          5378.655443999879,
          5491.508797368228,
          5593.810107962466,
          5687.365990700164,
          5773.550144757456,
          5853.432915032118,
          5927.865353721229
         ],
         "yaxis": "y"
        },
        {
         "hovertemplate": "order=5<br>gamma=%{x}<br>perplexity=%{y}<extra></extra>",
         "legendgroup": "5",
         "line": {
          "color": "#FFA15A",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "5",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0.1,
          0.2,
          0.3,
          0.4,
          0.5,
          0.6,
          0.7,
          0.8,
          0.9,
          1,
          1.1,
          1.2,
          1.3,
          1.4
         ],
         "xaxis": "x",
         "y": [
          4330.841988098133,
          4754.727143156697,
          5037.931465461194,
          5254.5031695012185,
          5431.010407129221,
          5580.418686880007,
          5710.140187825957,
          5824.851136587415,
          5927.708610723096,
          6020.952648953494,
          6106.2343228778145,
          6184.807874711034,
          6257.6497583447335,
          6325.5358016390455
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "#DDA15E",
          "size": 12
         },
         "mode": "markers",
         "name": "Min Perplexity",
         "type": "scatter",
         "x": [
          0.1
         ],
         "y": [
          791.4323585225626
         ]
        }
       ],
       "layout": {
        "height": 600,
        "legend": {
         "title": {
          "text": "order"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Broad Grid Search with Lidstone"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "gamma"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "perplexity"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.line(results_df, x='gamma', y='perplexity', color='order', \n",
    "              color_discrete_sequence=px.colors.qualitative.Plotly,\n",
    "              title = \"Broad Grid Search with Lidstone\")\n",
    "fig.add_trace(go.Scatter(x=[results_df_slice.loc[best_index,'gamma']], y=[results_df_slice.loc[best_index,'perplexity']], mode='markers', marker=dict(color='#DDA15E', size=12),name='Min Perplexity'))\n",
    "fig.update_layout(width=1000, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With broader search, we find the trends in the perplexity of each $n$ and $\\gamma$, we would pursue the search on the left hand-side, where gamma is closer to 0.00 and with order = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Focused Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This time we want to focus on $n$ = 2, and check the smallest $\\gamma$ possible around 0 and 0, every 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamters Init\n",
    "order_max = 2\n",
    "gamma_min = 0\n",
    "gamma_max = 0.2\n",
    "gamma_increment = 0.01\n",
    "\n",
    "# Score Init\n",
    "best_model_perplexity = 10**10\n",
    "best_model_order = 0\n",
    "best_model_gamma = 0\n",
    "\n",
    "# Store each Values\n",
    "results_array = np.empty((0, 3))\n",
    "\n",
    "# Grid Search\n",
    "for order in range(2,order_max+1):\n",
    "\n",
    "    for gamma in np.arange(gamma_min+gamma_increment, gamma_max, gamma_increment).round(10):\n",
    "\n",
    "        model = train_language_models_grid(n = order, corpus = Train_Corpus,  model_type = \"lidstone\", gamma = gamma)\n",
    "        \n",
    "        perplexity = model.perplexity([tuple(ngram) for sentence in Test_Corpus for ngram in ngrams(pad_both_ends(sentence, n=order), n=order)])\n",
    "\n",
    "        new_row = np.array([[order, gamma, perplexity]])\n",
    "        results_array = np.append(results_array, new_row, axis=0)\n",
    "\n",
    "        if perplexity < best_model_perplexity:\n",
    "            best_model_perplexity = perplexity\n",
    "            best_model_order = order\n",
    "            best_model_gamma = gamma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Hyperparameters and Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_15d6a_row0_col0, #T_15d6a_row0_col1, #T_15d6a_row0_col2 {\n",
       "  background-color: #a87d4c;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_15d6a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_15d6a_level0_col0\" class=\"col_heading level0 col0\" >order</th>\n",
       "      <th id=\"T_15d6a_level0_col1\" class=\"col_heading level0 col1\" >gamma</th>\n",
       "      <th id=\"T_15d6a_level0_col2\" class=\"col_heading level0 col2\" >perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_15d6a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_15d6a_row0_col0\" class=\"data row0 col0\" >2</td>\n",
       "      <td id=\"T_15d6a_row0_col1\" class=\"data row0 col1\" >0.010000</td>\n",
       "      <td id=\"T_15d6a_row0_col2\" class=\"data row0 col2\" >484.943266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15d6a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_15d6a_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_15d6a_row1_col1\" class=\"data row1 col1\" >0.020000</td>\n",
       "      <td id=\"T_15d6a_row1_col2\" class=\"data row1 col2\" >529.076891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15d6a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_15d6a_row2_col0\" class=\"data row2 col0\" >2</td>\n",
       "      <td id=\"T_15d6a_row2_col1\" class=\"data row2 col1\" >0.030000</td>\n",
       "      <td id=\"T_15d6a_row2_col2\" class=\"data row2 col2\" >570.386228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15d6a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_15d6a_row3_col0\" class=\"data row3 col0\" >2</td>\n",
       "      <td id=\"T_15d6a_row3_col1\" class=\"data row3 col1\" >0.040000</td>\n",
       "      <td id=\"T_15d6a_row3_col2\" class=\"data row3 col2\" >608.205574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15d6a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_15d6a_row4_col0\" class=\"data row4 col0\" >2</td>\n",
       "      <td id=\"T_15d6a_row4_col1\" class=\"data row4 col1\" >0.050000</td>\n",
       "      <td id=\"T_15d6a_row4_col2\" class=\"data row4 col2\" >643.212079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_15d6a_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_15d6a_row5_col0\" class=\"data row5 col0\" >2</td>\n",
       "      <td id=\"T_15d6a_row5_col1\" class=\"data row5 col1\" >0.060000</td>\n",
       "      <td id=\"T_15d6a_row5_col2\" class=\"data row5 col2\" >675.974686</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x281c74e20>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def highlight_lowest_perplexity(row):\n",
    "    if row['perplexity'] == results_df['perplexity'].min():\n",
    "        return ['background-color: #a87d4c'] * len(row)\n",
    "    else:\n",
    "        return [''] * len(row)\n",
    "    \n",
    "results_df = pd.DataFrame(results_array, columns=['order', 'gamma', 'perplexity'])\n",
    "results_df.style.apply(highlight_lowest_perplexity, axis=1)\n",
    "results_df[\"order\"] = results_df[\"order\"].astype(int)\n",
    "best_index = results_df['perplexity'].idxmin()\n",
    "\n",
    "# Select the 10 rows around the best row\n",
    "slice_start = max(0, best_index - 5)\n",
    "slice_end = min(len(results_df), best_index + 6)\n",
    "results_df_slice = results_df.iloc[slice_start:slice_end]\n",
    "\n",
    "# Apply the styling function to the sliced DataFrame\n",
    "styled_df_slice = results_df_slice.style.apply(highlight_lowest_perplexity, axis=1)\n",
    "\n",
    "# Display the styled DataFrame\n",
    "styled_df_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Highlighted row in this table is our best Perplexity on Test, which shows that Lidstone of $n$ = 2 and $\\gamma$ = 0.01 seems the best for our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "order=2<br>gamma=%{x}<br>perplexity=%{y}<extra></extra>",
         "legendgroup": "2",
         "line": {
          "color": "#636EFA",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "2",
         "orientation": "v",
         "showlegend": true,
         "type": "scatter",
         "x": [
          0.01,
          0.02,
          0.03,
          0.04,
          0.05,
          0.06,
          0.07,
          0.08,
          0.09,
          0.1,
          0.11,
          0.12,
          0.13,
          0.14,
          0.15,
          0.16,
          0.17,
          0.18,
          0.19
         ],
         "xaxis": "x",
         "y": [
          484.9432656886147,
          529.0768914081395,
          570.3862275401669,
          608.2055743539142,
          643.2120786323194,
          675.9746861700355,
          706.9091368686388,
          736.3200592975144,
          764.435910254543,
          791.4323585225626,
          817.4476075803065,
          842.5925926321493,
          866.9579304572644,
          890.6187746883514,
          913.6382877898194,
          936.0701772704108,
          957.960584274363,
          979.3495144471897,
          1000.2719390079328
         ],
         "yaxis": "y"
        },
        {
         "marker": {
          "color": "#DDA15E",
          "size": 12
         },
         "mode": "markers",
         "name": "Min Perplexity",
         "type": "scatter",
         "x": [
          0.01
         ],
         "y": [
          484.9432656886147
         ]
        }
       ],
       "layout": {
        "height": 600,
        "legend": {
         "title": {
          "text": "order"
         },
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Focused Grid Search with Lidstone"
        },
        "width": 1000,
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "gamma"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "perplexity"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = px.line(results_df, x='gamma', y='perplexity', color='order', \n",
    "              color_discrete_sequence=px.colors.qualitative.Plotly,\n",
    "              title = \"Focused Grid Search with Lidstone\")\n",
    "fig.update_layout(width=1000, height=600)\n",
    "fig.add_trace(go.Scatter(x=[results_df_slice.loc[best_index,'gamma']], y=[results_df_slice.loc[best_index,'perplexity']], mode='markers', marker=dict(color='#DDA15E', size=12),name='Min Perplexity'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The optimal perplexity appears to hover around 484, achieved with $\\gamma$ = 0.01. This confirms the effectiveness of our Broad Search approach, emphasizing lower values of $\\gamma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chosen Model for better Generalisation on new Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Lidstone* with $n$ = 2 and $\\gamma$ = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Vocabulary with cutoff=1 unk_label='<UNK>' and 12136 items> \n",
      "with gamma: 0.01 \n",
      "of order: 2\n"
     ]
    }
   ],
   "source": [
    "model_final = train_language_models_grid(n = 2, corpus = Train_Corpus,  model_type = \"lidstone\", gamma = 0.01)\n",
    "print(model_final.vocab, \"\\nwith gamma:\",model_final.gamma, \"\\nof order:\", model_final.order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The order of 2 or 3 was expected, given the brevity of most Trump tweets, often relying on punchlines and key words like hashtags or names. It appears that the 2-gram pattern performs better on the Test set, with the benefit of a small $\\gamma$ as a smoothing parameter.\n",
    "> With Lidstone $n$ = 2 and $\\gamma$ = 0.01, the achieved Perplexity stands at 484, indicating the model's hesitation among 484 words within a vocabulary set of 12,136 words. This represents roughly 3% of the trained tokens, a promising result. We're constantly navigating the trade-off between Bias and Variance in our model. By tuning the flexible parameter $\\gamma$, we control the balance between introducing bias and reducing variance on a Test set. If we aim for fewer hesitations on new words, we decrease variance but at the expense of introducing bias.\n",
    "> Perplexity, though useful, has limitations in assessing Language Learning Models (LLMs). It tends to prioritize immediate context over broader understanding, overlook ambiguity and creativity, and is sensitive to vocabulary size. Furthermore, achieving low perplexity doesn't guarantee effective generalization to real-world data. \n",
    "> \n",
    "> (Sourabh (2023). Decoding Perplexity and its significance in LLMs. [online] UpTrain AI. Available at: https://blog.uptrain.ai/decoding-perplexity-and-its-significance-in-llms/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "## <center> END </center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
